{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery, QueryType, QueryCaptionType, QueryAnswerType, VectorFilterMode\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "key = os.getenv(\"AZURE_SEARCH_KEY\") \n",
    "false = True #Set to true to see more output information\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "model = \"ada002\"\n",
    "\n",
    "#Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_KEY'],  \n",
    "  api_version = \"2023-12-01-preview\"\n",
    "  )\n",
    "\n",
    "messages=[]\n",
    "\n",
    "def count_tokens(prompt) -> int:  \n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    token_sizes = len(encoding.encode(prompt))\n",
    "    return token_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_content=\"\"\n",
    "# system message\n",
    "system_message = f'''\n",
    "You are an assistant with knowledge of the following topics:\n",
    "1. IEC61131-3 languages\n",
    "2. Structured Text\n",
    "3. Function Block Diagram\n",
    "4. IEC61131-3 coding standards\n",
    "5. IEC61131-3 best practices\n",
    "6. IEC61131-3 coding guidelines\n",
    "7. IEC61131-3 programming\n",
    "8. IEC61131-3 programming languages\n",
    "9. Schneider Electric EcoStruxure Control Expert\n",
    "10. Schneider Electric EcoStruxure Machine Expert\n",
    "11. Schneider Electric EcoStruxure Machine Expert Libraries and Templates\n",
    "\n",
    "Your job is to generate small examples of code using exclusiveliy IEC61131-3 Structured Text base on user input.\n",
    "You can assume that all the code will be executed on a Schneider Electric EcoStruxure Control Expert or Schneider Electric EcoStruxure Machine Expert PLC and that all libraries are available.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_command(lib_name:str, fn_name:str, rag_info:str):\n",
    "  return f'''  \n",
    "    Generate a small program in IEC61131-3 that uses Scheneider Electric {lib_name} library to send an email using following parameters:\n",
    "    1. To: \"receiver@se.com\"\n",
    "    2. Subject: \"Test email\"\n",
    "    3. Body: \"This is a test email\"\n",
    "    4. From: \"sender@se.com\"\n",
    "\n",
    "    Authentication required.\n",
    "    Message should be sent with high priority.\n",
    "    Use Login to authenticate using \"corrado\" as user and \"p@ssw0rd123\" as password.\n",
    "    Verify that the email has been sent successfully, if not print the error message.\n",
    "\n",
    "    These are the information about the {fn_name} function from the {lib_name} library you have to use:\n",
    "    ```\n",
    "    {rag_info}\n",
    "    ```\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell uses LLM to summarize RAG content, but looks like is not working since it strips out most of the important info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages=[]\n",
    "# messages.append({'role': 'system', 'content': \"You are an assistant expert in summarize code libraries documentation. Your goal is to summarize the user provided content removing not relevant information\"})\n",
    "# messages.append({'role': 'user', 'content': rag_content})  \n",
    "\n",
    "# token_count=count_tokens(rag_content);\n",
    "# print (f'Total input RAG tokens: {token_count}')     \n",
    "\n",
    "# openai_response = client.chat.completions.create(\n",
    "#         model=deployment,    \n",
    "#         messages = messages,\n",
    "#         temperature=0.3,\n",
    "#         max_tokens=800,\n",
    "#         top_p=0.95,\n",
    "#         frequency_penalty=0,\n",
    "#         presence_penalty=0,\n",
    "#         stop=None)\n",
    "\n",
    "# rag_content= openai_response.choices[0].message.content\n",
    "\n",
    "# token_count=count_tokens(rag_content);\n",
    "# print (f'Summarized RAG tokens: {token_count}') \n",
    "# messages=[]\n",
    "\n",
    "# # system message\n",
    "# system_message = f'''\n",
    "# You are an assistant with knowledge of the following topics:\n",
    "# 1. IEC61131-3 languages\n",
    "# 2. Structured Text\n",
    "# 3. Function Block Diagram\n",
    "# 4. IEC61131-3 coding standards\n",
    "# 5. IEC61131-3 best practices\n",
    "# 6. IEC61131-3 coding guidelines\n",
    "# 7. IEC61131-3 programming\n",
    "# 8. IEC61131-3 programming languages\n",
    "# 9. Schneider Electric EcoStruxure Control Expert\n",
    "# 10. Schneider Electric EcoStruxure Machine Expert\n",
    "# 11. Schneider Electric EcoStruxure Machine Expert Libraries and Templates\n",
    "\n",
    "# Your job is to generate small examples of code using exclusiveliy IEC61131-3 Structured Text base on user input.\n",
    "# You can assume that all the code will be executed on a Schneider Electric EcoStruxure Control Expert or Schneider Electric EcoStruxure Machine Expert PLC and that all libraries are available.\n",
    "\n",
    "# Use the following pieces of retrieved context to answer the question.\n",
    "# CONTEXT:\n",
    "# ```\n",
    "# {rag_content}\n",
    "# ```\n",
    "\n",
    "# '''\n",
    "\n",
    "# print(system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_embeddings(text):\n",
    "    return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "def get_from_RAG(query: str, max_results: int = 10, verbose: bool = False):    \n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(vector= generate_embeddings(query), k_nearest_neighbors=max_results, fields=\"embedding\")\n",
    "    results = search_client.search(          \n",
    "        vector_queries=[vector_query],\n",
    "        select=[\"id\",\"sourcefile\",\"content\"],\n",
    "        query_type=QueryType.SEMANTIC, \n",
    "        top=max_results,\n",
    "        semantic_configuration_name='default', \n",
    "        query_caption=QueryCaptionType.EXTRACTIVE, \n",
    "        query_answer=QueryAnswerType.EXTRACTIVE        \n",
    "    )   \n",
    "    \n",
    "    rag_results=[]\n",
    "    if results :        \n",
    "        for result in results:\n",
    "            if verbose:            \n",
    "                print(f\"Id: {result['id']}\")\n",
    "                print(f\"Reranker Score: {result['@search.score']}\")\n",
    "                print(f\"Content: {result['content']}\")\n",
    "                print(f\"Sourcefile: {result['sourcefile']}\")                \n",
    "            rag_results.append(result['content'])                \n",
    "                \n",
    "    return rag_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses are the user commands, edit and run them to see the different output considering that output also depends on code implemented into function (to be replaced by RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rag tokens: 1958\n",
      "Here is a small program in IEC61131-3 Structured Text that uses the Schneider Electric EmailHandling library to send an email with the specified parameters:\n",
      "\n",
      "```iecst\n",
      "PROGRAM SendEmail\n",
      "VAR\n",
      "    EmailHandler: FB_SendEMail;\n",
      "    Credentials: ST_CredentialsSendEMail;\n",
      "    Email: ST_EMail;\n",
      "    Result: ET_Result;\n",
      "END_VAR\n",
      "```\n",
      "```iecst\n",
      "// Set up the credentials\n",
      "Credentials.sUser := 'corrado';\n",
      "Credentials.sPassword := 'p@ssw0rd123';\n",
      "\n",
      "// Set up the email\n",
      "Email.sFrom := 'sender@se.com';\n",
      "Email.sTo := 'receiver@se.com';\n",
      "Email.sSubject := 'Test email';\n",
      "Email.sBody := 'This is a test email';\n",
      "Email.ePriority := ET_Priority#High;\n",
      "\n",
      "// Send the email\n",
      "EmailHandler.iq_stCredentials := Credentials;\n",
      "EmailHandler.iq_stEMail := Email;\n",
      "EmailHandler.i_xEnable := TRUE;\n",
      "\n",
      "// Execute the function block\n",
      "EmailHandler();\n",
      "\n",
      "// Check the result\n",
      "Result := EmailHandler.q_etResult;\n",
      "IF Result <> ET_Result#Ok THEN\n",
      "    // Print the error message\n",
      "    PRINTF('Failed to send email: %s', EmailHandler.q_sError);\n",
      "END_IF\n",
      "END_PROGRAM\n",
      "```\n",
      "\n",
      "This program first sets up the credentials and the email content. It then enables the `FB_SendEMail` function block and executes it. After the function block has been executed, it checks the result and prints an error message if the email was not sent successfully.\n",
      "\n",
      "Please note that this is a basic example and does not include error handling or retries. In a real-world application, you would likely want to add additional logic to handle errors and retry sending the email if necessary.\n"
     ]
    }
   ],
   "source": [
    "lib_name=\"EmailHandling\"\n",
    "fn_name=\"FB_SendEmail\"\n",
    "\n",
    "query = f\"Return all the info related to the {fn_name} function in the {lib_name} library.\"\n",
    "rag_results = get_from_RAG(query, max_results=10, verbose=false)\n",
    "rag_data='n'.join(rag_results)\n",
    "\n",
    "token_count=count_tokens(rag_data);\n",
    "print (f'Total rag tokens: {token_count}')  \n",
    "\n",
    "# uncomment to remove rag_data\n",
    "#rag_data=\"\"\n",
    "  \n",
    "user_command=get_user_command(lib_name, fn_name, rag_data)\n",
    "\n",
    "messages.append({'role': 'system', 'content': system_message})\n",
    "messages.append({'role': 'user', 'content': user_command})  \n",
    "\n",
    "openai_response = client.chat.completions.create(\n",
    "        model=deployment,    \n",
    "        messages = messages,\n",
    "        temperature=0.3,\n",
    "        max_tokens=800,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None)\n",
    "result= openai_response.choices[0].message\n",
    "\n",
    "print (result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptflow",
   "language": "python",
   "name": "promptflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
