{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Environment setup\n",
    "load_dotenv()\n",
    "\n",
    "deployment=os.environ['AZURE_OPENAI_DEPLOYMENT']\n",
    "key = os.getenv(\"AZURE_SEARCH_KEY\") \n",
    "verbose = False #Set to true to see more output information\n",
    "\n",
    "#Initialize AzureOpenAI client\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ['AZURE_OPENAI_KEY'],  \n",
    "  api_version = \"2023-12-01-preview\"\n",
    "  )\n",
    "\n",
    "messages=[]\n",
    "\n",
    "def count_tokens(prompt) -> int:  \n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "    token_sizes = len(encoding.encode(prompt))\n",
    "    return token_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system message\n",
    "system_message = f'''\n",
    "You are an assistant in charge of gathering the correctness and completion of provide informations from the user and, in case they are not provide a set of question to be asked to the user.\n",
    "If all the requisites are provided, please reply with the action to be executed including the name of the libraries to be used.\n",
    "To select the libraries to be used, please use the following list:\n",
    "  - To open or close something: use the library \"SE_OPEN_CLOSE\"\n",
    "  - To activate or deactivate something: use the library \"SE_ACTIVATE_DEACTIVATE\"\n",
    "  - To load or trigger something: use the library \"SE_LOAD_TRIGGER\"\n",
    "  - To cut something: use the library \"SE_CUT\"\n",
    "  - To print something: use the library \"SE_PRINT_262\" for machine \"M262\" and \"SE_PRINT_262\" for machine \"M263\"\n",
    "\n",
    "\n",
    "These are the input requirements to be verified:\n",
    "\n",
    "1. The user must provide one or more actions to be executed (e.g. opening, closing, activating, load, trigger and so on).\n",
    "2. If no action indicated please reply \"What action do you want to perform?\"\n",
    "3. The user must provide one or more objects that the action will operate on (e.g. material, light, fan, and so on).\n",
    "4. The user has to indicate the name of the PLC to use, if not provided please:\n",
    "  - Reply with \"What is the name of the PLC to use?\"\n",
    "  - Do not ask any other question until an the PLC name is provided.\n",
    "4. If action is cut the following info have to be provided:\n",
    "  - The lenght of the cut\n",
    "  - How wide is the cut\n",
    "  - The depth of the cut\n",
    "  - The material to be cut\n",
    "5. If action is to print someting the following info have to be provided: \n",
    "  - What has to be printed\n",
    "  - Where the content has to be printed\n",
    "  - The material to be printed\n",
    "  - The color of the print\n",
    "  - The size of the print\n",
    "  - The number of copies\n",
    "\n",
    "\n",
    "In the reply don't include any other text/info other than question regarding the missing prerequisites to fullfill the requisites.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_command = f'''\n",
    "Hi, how are you doing today?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theses are the user commands, edit and run them to see the different output considering that output also depends on code implemented into function (to be replaced by RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> User: Hello\n",
      "---->LLM: What action do you want to perform?\n",
      "-> User: i want to print\n",
      "---->LLM: What has to be printed? Where should the content be printed? What is the material to be printed on? What is the color of the print? What is the size of the print? How many copies do you need? What is the name of the PLC to use?\n",
      "-> User: \n",
      "---->LLM: What is the name of the PLC to use?\n"
     ]
    }
   ],
   "source": [
    "messages.append({'role': 'system', 'content': system_message})\n",
    "llm_response=\"\"\n",
    "\n",
    "while True:\n",
    "        \n",
    "        user_command = input()\n",
    "        if user_command == \"exit\":\n",
    "            break\n",
    "        if user_command != \"\":\n",
    "            print(f\"-> User: {user_command}\", flush=True)\n",
    "            messages.append({'role': 'user', 'content': user_command})   \n",
    "        \n",
    "        openai_response = client.chat.completions.create(\n",
    "                model=deployment,    \n",
    "                messages = messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=800,\n",
    "                top_p=0.95,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=None)\n",
    "        result= openai_response.choices[0].message\n",
    "        llm_response = result.content  \n",
    "        print(f\"---->LLM: {llm_response}\", flush=True)\n",
    "        messages.append({'role': 'assistant', 'content': result.content})     \n",
    "        time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "promptflow",
   "language": "python",
   "name": "promptflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
